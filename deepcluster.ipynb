{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用代码实现下面场景的需求。\n",
    "\n",
    "场景：\n",
    "现有一批文本数据，只有少部分有标记。使用对比学习的深度聚类进行训练。对比学习使用基于同义词替换的数据增强方式完成样本构造。\n",
    "在一个mini-batch中，对于样本x1来说正样本为其增强后的样本x1_,负样本为该mini-batch中其他样本x2及其增强后的样本x2_。\n",
    "损失函数包括对比损失（InfoNCE），聚类损失（KL散度）和纯净度损失（即聚类后属于同一个类别的簇包含不同真实标签数据的多少。\n",
    "若该簇没有真实标签的数据或有真实标签的数据但都属于同一类别，则该损失为0，否则有越多不同类别的真实标签数据损失越大）。\n",
    "\n",
    "方法思路\n",
    "1. 划分数据：\n",
    "从有标签数据按各类别的20%比例划分出验证集V1,剩余的标记数据未L和未标记数据N。\n",
    "\n",
    "2. 初始化阶段：\n",
    "使用标记数据L训练微调一个bert语言模型。\n",
    "\n",
    "3. 伪标签生成：\n",
    "用微调后的语言模型提取所有数据(L+N)的词嵌入特征，然后用k-means进行聚类，给所有数据打上伪标签。\n",
    "\n",
    "4. 打真实标签：\n",
    "遍历聚类结果，计算每一个类别的置信度（该类中具有某类真实标签数据的数量比上该类中具有真实标签数据的数量中的最大值），给置信度大于50%且数量大于3的类别打上该类的真实标签，此时L扩充为L=L+m，N缩减为N=N-m。\n",
    "\n",
    "5.循环执行：\n",
    "重复步骤2-4，直到所有N都被打上真实标签或达到最大的迭代次数。\n",
    "\n",
    "6. 最终评估：\n",
    "在独立的验证集V1上评估模型的最终性能，确保模型的有效性和泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据增强\n",
    "class TextAugmenter:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def synonym_replace(self, text):\n",
    "        tokens = text.split()\n",
    "        n = max(1, int(len(tokens) * 0.1))  # 替换10%的词\n",
    "        positions = random.sample(range(len(tokens)), n)\n",
    "        \n",
    "        for pos in positions:\n",
    "            word = tokens[pos]\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "            if synonyms:\n",
    "                tokens[pos] = random.choice(synonyms)\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# 自定义数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, augmenter):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augmenter = augmenter\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 原始样本\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', \n",
    "                                truncation=True, max_length=128)\n",
    "        \n",
    "        # 增强样本\n",
    "        aug_text = self.augmenter.synonym_replace(text)\n",
    "        aug_encoding = self.tokenizer(aug_text, return_tensors='pt', padding='max_length',\n",
    "                                    truncation=True, max_length=128)\n",
    "        \n",
    "        return encoding, aug_encoding, label\n",
    "\n",
    "# 模型定义\n",
    "class ContrastiveClusterModel(nn.Module):\n",
    "    def __init__(self, n_clusters):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.cluster_head = nn.Linear(128, n_clusters)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        features = self.projector(embeddings)\n",
    "        logits = self.cluster_head(features)\n",
    "        return features, logits\n",
    "\n",
    "# 损失函数\n",
    "class ContrastiveClusterLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features, aug_features, cluster_pred, true_labels, pseudo_labels):\n",
    "        # InfoNCE loss\n",
    "        features = F.normalize(features, dim=1)\n",
    "        aug_features = F.normalize(aug_features, dim=1)\n",
    "        \n",
    "        pos_sim = torch.sum(features * aug_features, dim=1)\n",
    "        neg_sim = torch.mm(features, features.t())\n",
    "        \n",
    "        nce_loss = -torch.log(\n",
    "            torch.exp(pos_sim / self.temperature) /\n",
    "            (torch.sum(torch.exp(neg_sim / self.temperature), dim=1) - 1)\n",
    "        ).mean()\n",
    "        \n",
    "        # Clustering loss (KL divergence)\n",
    "        cluster_pred = F.log_softmax(cluster_pred, dim=1)\n",
    "        q = self.target_distribution(cluster_pred)\n",
    "        cluster_loss = F.kl_div(cluster_pred, q, reduction='batchmean')\n",
    "        \n",
    "        # Purity loss\n",
    "        purity_loss = self.compute_purity_loss(cluster_pred, true_labels)\n",
    "        \n",
    "        return nce_loss + cluster_loss + purity_loss\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.t() / weight.sum(1)).t()\n",
    "    \n",
    "    def compute_purity_loss(self, cluster_pred, true_labels):\n",
    "        loss = 0\n",
    "        pred_labels = torch.argmax(cluster_pred, dim=1)\n",
    "        \n",
    "        for c in torch.unique(pred_labels):\n",
    "            cluster_mask = (pred_labels == c)\n",
    "            cluster_labels = true_labels[cluster_mask]\n",
    "            \n",
    "            if len(cluster_labels) == 0 or len(torch.unique(cluster_labels)) <= 1:\n",
    "                continue\n",
    "                \n",
    "            loss += len(torch.unique(cluster_labels)) - 1\n",
    "            \n",
    "        return loss / len(torch.unique(pred_labels))\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        orig_encoding, aug_encoding, labels = batch\n",
    "        \n",
    "        orig_encoding = {k: v.squeeze(1).to(device) for k, v in orig_encoding.items()}\n",
    "        aug_encoding = {k: v.squeeze(1).to(device) for k, v in aug_encoding.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        features, cluster_pred = model(**orig_encoding)\n",
    "        aug_features, _ = model(**aug_encoding)\n",
    "        \n",
    "        loss = criterion(features, aug_features, cluster_pred, labels, None)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabelGenerator:\n",
    "    def __init__(self, n_clusters, confidence_threshold=0.5, min_samples=3):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.min_samples = min_samples\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters)\n",
    "        \n",
    "    def extract_features(self, model, dataloader, device):\n",
    "        \"\"\"提取所有数据的特征\"\"\"\n",
    "        model.eval()\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        indices_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (orig_encoding, _, labels) in enumerate(dataloader):\n",
    "                orig_encoding = {k: v.squeeze(1).to(device) for k, v in orig_encoding.items()}\n",
    "                features, _ = model(**orig_encoding)\n",
    "                features_list.append(features.cpu().numpy())\n",
    "                labels_list.append(labels.numpy())\n",
    "                indices_list.extend(range(batch_idx * dataloader.batch_size,\n",
    "                                       min((batch_idx + 1) * dataloader.batch_size,\n",
    "                                           len(dataloader.dataset))))\n",
    "        \n",
    "        features = np.concatenate(features_list, axis=0)\n",
    "        labels = np.concatenate(labels_list, axis=0)\n",
    "        return features, labels, indices_list\n",
    "\n",
    "    def generate_pseudo_labels(self, features, true_labels, labeled_mask):\n",
    "        \"\"\"生成伪标签\"\"\"\n",
    "        # 使用K-means聚类\n",
    "        cluster_labels = self.kmeans.fit_predict(features)\n",
    "        \n",
    "        # 计算聚类质量\n",
    "        silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "        print(f\"Clustering Silhouette Score: {silhouette_avg:.4f}\")\n",
    "        \n",
    "        # 为每个簇计算置信度\n",
    "        cluster_info = self._analyze_clusters(cluster_labels, true_labels, labeled_mask)\n",
    "        \n",
    "        # 生成新的伪标签\n",
    "        pseudo_labels = np.full(len(features), -1)  # -1表示未分配标签\n",
    "        confident_indices = []\n",
    "        \n",
    "        for cluster_id, info in cluster_info.items():\n",
    "            if info['confidence'] >= self.confidence_threshold and info['count'] >= self.min_samples:\n",
    "                cluster_mask = (cluster_labels == cluster_id)\n",
    "                pseudo_labels[cluster_mask] = info['majority_label']\n",
    "                # 只为未标记数据生成伪标签\n",
    "                confident_indices.extend([i for i, (m, c) in enumerate(zip(~labeled_mask, cluster_mask))\n",
    "                                       if m and c])\n",
    "        \n",
    "        return pseudo_labels, confident_indices, cluster_labels\n",
    "\n",
    "    def _analyze_clusters(self, cluster_labels, true_labels, labeled_mask):\n",
    "        \"\"\"分析每个簇的标签分布和置信度\"\"\"\n",
    "        cluster_info = {}\n",
    "        \n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            cluster_mask = (cluster_labels == cluster_id)\n",
    "            labeled_in_cluster = labeled_mask & cluster_mask\n",
    "            \n",
    "            if not any(labeled_in_cluster):\n",
    "                cluster_info[cluster_id] = {\n",
    "                    'confidence': 0.0,\n",
    "                    'majority_label': -1,\n",
    "                    'count': np.sum(cluster_mask)\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # 统计该簇中有标签样本的类别分布\n",
    "            cluster_true_labels = true_labels[labeled_in_cluster]\n",
    "            label_counts = Counter(cluster_true_labels)\n",
    "            \n",
    "            majority_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
    "            majority_count = label_counts[majority_label]\n",
    "            total_labeled = sum(label_counts.values())\n",
    "            \n",
    "            cluster_info[cluster_id] = {\n",
    "                'confidence': majority_count / total_labeled,\n",
    "                'majority_label': majority_label,\n",
    "                'count': np.sum(cluster_mask)\n",
    "            }\n",
    "        \n",
    "        return cluster_info\n",
    "\n",
    "def update_dataset(dataset, pseudo_labels, confident_indices):\n",
    "    \"\"\"更新数据集的标签\"\"\"\n",
    "    for idx in confident_indices:\n",
    "        dataset.labels[idx] = pseudo_labels[idx]\n",
    "    return dataset\n",
    "\n",
    "def train_with_pseudo_labels(model, train_dataset, device, max_epochs=10, pseudo_label_interval=2):\n",
    "    \"\"\"带伪标签更新的训练循环\"\"\"\n",
    "    pseudo_label_generator = PseudoLabelGenerator(n_clusters=10)\n",
    "    criterion = ContrastiveClusterLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # 初始化数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # 记录已标记的样本\n",
    "    labeled_mask = np.array([label != -1 for label in train_dataset.labels])\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 常规训练\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # 定期更新伪标签\n",
    "        if (epoch + 1) % pseudo_label_interval == 0:\n",
    "            # 提取特征\n",
    "            features, true_labels, indices = pseudo_label_generator.extract_features(\n",
    "                model, train_loader, device)\n",
    "            \n",
    "            # 生成新的伪标签\n",
    "            pseudo_labels, confident_indices, cluster_labels = pseudo_label_generator.generate_pseudo_labels(\n",
    "                features, true_labels, labeled_mask)\n",
    "            \n",
    "            # 更新数据集\n",
    "            train_dataset = update_dataset(train_dataset, pseudo_labels, confident_indices)\n",
    "            \n",
    "            # 更新标记掩码\n",
    "            labeled_mask[confident_indices] = True\n",
    "            \n",
    "            print(f\"Generated {len(confident_indices)} new pseudo labels\")\n",
    "            print(f\"Total labeled samples: {np.sum(labeled_mask)}/{len(labeled_mask)}\")\n",
    "            \n",
    "            # 如果所有样本都被标记，提前结束\n",
    "            if np.all(labeled_mask):\n",
    "                print(\"All samples have been labeled. Stopping training.\")\n",
    "                break\n",
    "            \n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_dataset, device):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for orig_encoding, _, labels in val_loader:\n",
    "            orig_encoding = {k: v.squeeze(1).to(device) for k, v in orig_encoding.items()}\n",
    "            _, logits = model(**orig_encoding)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            correct += (predictions == labels.to(device)).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主训练循环\n",
    "def main():\n",
    "    # 初始化\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    augmenter = TextAugmenter(tokenizer)\n",
    "\n",
    "     # 准备数据集\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, stratify=labels)\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, augmenter)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer, augmenter)\n",
    "    \n",
    "    # 模型初始化\n",
    "    model = ContrastiveClusterModel(n_clusters=10).to(device)\n",
    "\n",
    "    # 训练模型\n",
    "    model = train_with_pseudo_labels(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        device=device,\n",
    "        max_epochs=10,\n",
    "        pseudo_label_interval=2\n",
    "    )\n",
    "    \n",
    "    # 评估模型\n",
    "    evaluate(model, val_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
