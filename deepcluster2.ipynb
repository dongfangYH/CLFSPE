{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "场景：\n",
    "现有一批文本数据，只有少部分有标记。使用对比学习的深度聚类进行训练。对比学习使用基于同义词替换的数据增强方式完成样本构造。\n",
    "在一个mini-batch中，对于样本x1来说正样本为其增强后的样本x1_,负样本为该mini-batch中其他样本x2及其增强后的样本x2_。\n",
    "损失函数包括对比损失（InfoNCE），聚类损失（KL散度）和纯度损失（即聚类后属于同一个类别的簇包含不同真实标签数据的多少。\n",
    "若该簇没有真实标签的数据或有真实标签的数据但都属于同一类别，则该损失为0，否则有越多不同类别的真实标签数据损失越大）。\n",
    "\n",
    "方法思路\n",
    "1. 划分数据：\n",
    "从有标签数据按各类别的20%比例划分出验证集V1,剩余的标记数据未L和未标记数据N。\n",
    "\n",
    "2. 初始化阶段：\n",
    "使用标记数据L训练微调一个bert语言模型。\n",
    "\n",
    "3. 生成真实标签并更新数据集：\n",
    "用微调后的语言模型进行预测对未标签数据N打标签，只有预测置信度大于阈值50%才打上真实标签。更新标签后的数据集，L扩充为L=L+m，N缩减为N=N-m。\n",
    "\n",
    "4. 聚类打类别标签：\n",
    "用语言模型提取所有数据(L+N)的词嵌入特征，然后用GMM进行聚类，给所有数据打上类别标签。\n",
    "\n",
    "5. 计算损失：\n",
    "计算对比损失，聚类损失和纯度损失。\n",
    "\n",
    "6. 循环执行：\n",
    "重复步骤3-5，直到所有N都被打上真实标签或达到最大的迭代次数或损失不再显著下降。\n",
    "\n",
    "7. 最终评估：\n",
    "在独立的验证集V1上评估模型的最终性能，确保模型的有效性和泛化能力。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "        \"This is a positive review\",\n",
    "        \"I don't like this product\",\n",
    "        \"Great service and quality\",\n",
    "        \"The worst experience ever\",\n",
    "        \"Amazing product, highly recommend\",\n",
    "        \"This is just okay\",\n",
    "        \"The prouduct is really good\",\n",
    "        \"I buy this product again\",\n",
    "        \"Good quality!\",\n",
    "        \"Not bad, Not bad\",\n",
    "        \"It works fine at present\",\n",
    "        \"It isn't out of my expectation\",\n",
    "        \"It's no hurt to give it a try\",\n",
    "        \"I don't think i will buy it again\",\n",
    "        \"I didn't use it a lot because i have thrown it away\",\n",
    "        \"who will buy this product?\",\n",
    "        # 无标签数据\n",
    "        \"Need to try this again\",\n",
    "        \"Not sure about this one\",\n",
    "        \"Will update my review later\",\n",
    "        \"I love it\",\n",
    "        \"It saves time\"\n",
    "]\n",
    "    \n",
    "# 部分标签（0：负面，1：中性，2：正面），None表示无标签\n",
    "labels = [2, 0, 2, 0, 2, 1, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, None, None, None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from typing import List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置类\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.max_length = 128\n",
    "        self.batch_size = 32\n",
    "        self.n_clusters = 10\n",
    "        self.learning_rate = 2e-5\n",
    "        self.max_epochs = 10\n",
    "        self.temperature = 0.07\n",
    "        self.confidence_threshold = 0.5\n",
    "        self.num_workers = 4\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "        self.gamma = 1\n",
    "\n",
    "# 数据增强\n",
    "class TextAugmenter:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def synonym_replace(self, text):\n",
    "        tokens = text.split()\n",
    "        n = max(1, int(len(tokens) * 0.1))  # 替换10%的词\n",
    "        positions = random.sample(range(len(tokens)), n)\n",
    "        \n",
    "        for pos in positions:\n",
    "            word = tokens[pos]\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "            if synonyms:\n",
    "                tokens[pos] = random.choice(synonyms)\n",
    "        \n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "# 自定义数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, augmenter, config):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augmenter = augmenter\n",
    "        self.max_length = config.max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 原始样本\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', \n",
    "                                truncation=True, max_length=self.max_length)\n",
    "        if self.augmenter:\n",
    "            # 增强样本\n",
    "            aug_text = self.augmenter.synonym_replace(text)\n",
    "            aug_encoding = self.tokenizer(aug_text, return_tensors='pt', padding='max_length',\n",
    "                                    truncation=True, max_length=self.max_length)\n",
    "            return encoding, aug_encoding, label\n",
    "        else:\n",
    "            return encoding, label\n",
    "    \n",
    "# 模型定义\n",
    "class ContrastiveClusterModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.cluster_head = nn.Linear(128, config.n_clusters)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        features = self.projector(embeddings)\n",
    "        logits = self.cluster_head(features)\n",
    "        return features, logits\n",
    "    \n",
    "# 损失函数\n",
    "class ContrastiveClusterLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.temperature = config.temperature\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.gamma = config.gamma\n",
    "        \n",
    "    def forward(self, features, aug_features, cluster_pred, true_labels, pseudo_labels):\n",
    "        # InfoNCE loss\n",
    "        features = F.normalize(features, dim=1)\n",
    "        aug_features = F.normalize(aug_features, dim=1)\n",
    "        \n",
    "        pos_sim = torch.sum(features * aug_features, dim=1)\n",
    "        neg_sim = torch.mm(features, features.t())\n",
    "        \n",
    "        nce_loss = -torch.log(\n",
    "            torch.exp(pos_sim / self.temperature) /\n",
    "            (torch.sum(torch.exp(neg_sim / self.temperature), dim=1) - 1)\n",
    "        ).mean()\n",
    "        \n",
    "        # Clustering loss (KL divergence)\n",
    "        cluster_pred = F.log_softmax(cluster_pred, dim=1)\n",
    "        q = self.target_distribution(cluster_pred)\n",
    "        cluster_loss = F.kl_div(cluster_pred, q, reduction='batchmean')\n",
    "        \n",
    "        # Purity loss\n",
    "        purity_loss = self.compute_purity_loss(cluster_pred, true_labels)\n",
    "        \n",
    "        return self.alpha * nce_loss + self.beta * cluster_loss + self.gamma * purity_loss\n",
    "    \n",
    "    def target_distribution(self, q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.t() / weight.sum(1)).t()\n",
    "    \n",
    "    def compute_purity_loss(self, cluster_pred, true_labels):\n",
    "        loss = 0\n",
    "        pred_labels = torch.argmax(cluster_pred, dim=1)\n",
    "        \n",
    "        for c in torch.unique(pred_labels):\n",
    "            cluster_mask = (pred_labels == c)\n",
    "            cluster_labels = true_labels[cluster_mask]\n",
    "            \n",
    "            if len(cluster_labels) == 0 or len(torch.unique(cluster_labels)) <= 1:\n",
    "                continue\n",
    "                \n",
    "            loss += len(torch.unique(cluster_labels)) - 1\n",
    "            \n",
    "        return loss / len(torch.unique(pred_labels))\n",
    "    \n",
    "\n",
    "def evaluate(model, val_dataset, config):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for orig_encoding, labels in val_loader:\n",
    "            orig_encoding = {k: v.squeeze(1).to(config.device) for k, v in orig_encoding.items()}\n",
    "            _, logits = model(**orig_encoding)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            correct += (predictions == labels.to(config.device)).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# 数据加载和划分函数\n",
    "def prepare_data(texts: List[str], labels: Optional[List[int]], \n",
    "                val_ratio: float = 0.2) -> Tuple:\n",
    "    \"\"\"\n",
    "    准备训练数据，划分验证集\n",
    "    \"\"\"\n",
    "    if labels is not None:\n",
    "        # 有标签数据划分\n",
    "        labeled_texts = []\n",
    "        labeled_labels = []\n",
    "        unlabeled_texts = []\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            if label is not None and not pd.isna(label):\n",
    "                labeled_texts.append(text)\n",
    "                labeled_labels.append(label)\n",
    "            else:\n",
    "                unlabeled_texts.append(text)\n",
    "                \n",
    "        # 划分验证集\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            labeled_texts, labeled_labels, \n",
    "            test_size=val_ratio, \n",
    "            stratify=labeled_labels\n",
    "        )\n",
    "\n",
    "        print(f\"数据集统计:\")\n",
    "        print(f\"训练集大小: {len(train_texts)}\")\n",
    "        print(f\"验证集大小: {len(val_texts)}\")\n",
    "        print(f\"未标记数据大小: {len(unlabeled_texts)}\")\n",
    "        \n",
    "        return train_texts, train_labels, unlabeled_texts, val_texts, val_labels\n",
    "    else:\n",
    "        # 全部为无标签数据\n",
    "        return [], [], texts, [], []\n",
    "    \n",
    "# 定义一个聚类器基类\n",
    "class BaseClusterer:\n",
    "    def fit_predict(self, features):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# GMM聚类实现\n",
    "class GMMClusterer(BaseClusterer):\n",
    "    def __init__(self, n_clusters):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gmm = GaussianMixture(n_components=n_clusters)\n",
    "        \n",
    "    def fit_predict(self, features):\n",
    "        # 将tensor转换为numpy\n",
    "        if torch.is_tensor(features):\n",
    "            features = features.cpu().detach().numpy()\n",
    "        return self.gmm.fit_predict(features)\n",
    "    \n",
    "def train_model(train_label_dataset, train_unlabel_dataset, config):\n",
    "    # 模型初始化\n",
    "    model = ContrastiveClusterModel(config).to(config.device)\n",
    "    criterion = ContrastiveClusterLoss(config)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # 初始化聚类器\n",
    "    clusterer = GMMClusterer(n_clusters=config.n_clusters)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 5\n",
    "    \n",
    "    for epoch in range(config.max_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.max_epochs}\")\n",
    "\n",
    "        # 创建数据加载器\n",
    "        label_loader = DataLoader(train_label_dataset, \n",
    "                            batch_size=config.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=config.num_workers)\n",
    "        unlabel_loader = DataLoader(train_unlabel_dataset,\n",
    "                              batch_size=config.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=config.num_workers)\n",
    "        \n",
    "        # 1. 提取所有数据特征\n",
    "        model.eval()\n",
    "        all_features = []\n",
    "        all_labels = []  # 有标签数据的真实标签\n",
    "        unlabel_features = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 提取有标签数据特征\n",
    "            for batch in label_loader:\n",
    "                orig_encoding, _, labels = batch\n",
    "                orig_encoding = {k: v.squeeze(1).to(config.device) for k, v in orig_encoding.items()}\n",
    "                features, _ = model(**orig_encoding)\n",
    "                all_features.append(features.cpu())\n",
    "                all_labels.extend(labels.numpy())\n",
    "            \n",
    "            # 提取无标签数据特征\n",
    "            for batch in unlabel_loader:\n",
    "                orig_encoding = batch[0]\n",
    "                orig_encoding = {k: v.squeeze(1).to(config.device) for k, v in orig_encoding.items()}\n",
    "                features, _ = model(**orig_encoding)\n",
    "                unlabel_features.append(features.cpu())\n",
    "\n",
    "        # 合并所有特征用于聚类\n",
    "        all_features = torch.cat(all_features, dim=0)\n",
    "        unlabel_features = torch.cat(unlabel_features, dim=0) if unlabel_features else torch.tensor([])\n",
    "        combined_features = torch.cat([all_features, unlabel_features], dim=0)\n",
    "        \n",
    "        # 2. GMM聚类\n",
    "        cluster_labels = clusterer.fit_predict(combined_features)\n",
    "        # 分离有标签和无标签数据的聚类结果\n",
    "        label_cluster_labels = cluster_labels[:len(all_features)]\n",
    "        unlabel_cluster_labels = cluster_labels[len(all_features):]\n",
    "\n",
    "        # 3. 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(label_loader):\n",
    "            orig_encoding, aug_encoding, labels = batch\n",
    "            \n",
    "            # 移动数据到设备\n",
    "            orig_encoding = {k: v.squeeze(1).to(config.device) for k, v in orig_encoding.items()}\n",
    "            aug_encoding = {k: v.squeeze(1).to(config.device) for k, v in aug_encoding.items()}\n",
    "            labels = labels.to(config.device)\n",
    "            \n",
    "            # 获取当前批次对应的聚类标签\n",
    "            start_idx = batch_idx * config.batch_size\n",
    "            end_idx = start_idx + len(labels)\n",
    "            batch_cluster_labels = torch.tensor(label_cluster_labels[start_idx:end_idx]).to(config.device)\n",
    "\n",
    "            # 前向传播\n",
    "            features_orig, logits_orig = model(**orig_encoding)\n",
    "            features_aug, logits_aug = model(**aug_encoding)\n",
    "\n",
    "            # 计算损失：对比损失 + 聚类损失 + 纯度损失\n",
    "            loss = criterion(\n",
    "                features=features_orig,\n",
    "                aug_features=features_aug,\n",
    "                cluster_pred=logits_orig,\n",
    "                true_labels=labels,\n",
    "                pseudo_labels=batch_cluster_labels  # 使用聚类结果作为伪标签\n",
    "            )\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # 更新学习率和早停\n",
    "        scheduler.step(avg_loss)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= max_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        # 4. 更新数据集\n",
    "        model.eval()\n",
    "        confidences = []\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in unlabel_loader:\n",
    "                orig_encoding = batch[0]\n",
    "                orig_encoding = {k: v.squeeze(1).to(config.device) for k, v in orig_encoding.items()}\n",
    "                _, logits = model(**orig_encoding)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                confidence, prediction = torch.max(probs, dim=1)\n",
    "                confidences.extend(confidence.cpu().numpy())\n",
    "                predictions.extend(prediction.cpu().numpy())\n",
    "\n",
    "        confident_mask = np.array(confidences) > config.confidence_threshold\n",
    "        if np.sum(confident_mask) > 0:\n",
    "            new_labeled_indices = np.where(confident_mask)[0]\n",
    "            new_labeled_texts = [train_unlabel_dataset.texts[i] for i in new_labeled_indices]\n",
    "            new_labeled_labels = [predictions[i] for i in new_labeled_indices]\n",
    "            \n",
    "            train_label_dataset.texts.extend(new_labeled_texts)\n",
    "            train_label_dataset.labels.extend(new_labeled_labels)\n",
    "            \n",
    "            remaining_indices = np.where(~confident_mask)[0]\n",
    "            train_unlabel_dataset.texts = [train_unlabel_dataset.texts[i] for i in remaining_indices]\n",
    "            \n",
    "            print(f\"Added {len(new_labeled_texts)} new labeled samples\")\n",
    "            print(f\"Remaining unlabeled samples: {len(train_unlabel_dataset)}\")\n",
    "\n",
    "        if len(train_unlabel_dataset) == 0:\n",
    "            print(\"All unlabeled data has been labeled\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "def count_unique_labels(lst):\n",
    "    series = pd.Series(lst)\n",
    "    filtered_series = series[pd.notna(series)]\n",
    "    return filtered_series.nunique()\n",
    "\n",
    "def main():\n",
    "    # 设置随机数种子\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "     # 初始化tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    augmenter = TextAugmenter(tokenizer)\n",
    "\n",
    "    # 准备数据\n",
    "    train_texts, train_labels, unlabeled_texts, val_texts, val_labels = prepare_data(\n",
    "        texts, labels\n",
    "    )\n",
    "\n",
    "    train_label_dataset = TextDataset(train_texts, train_labels, augmenter, config)\n",
    "    train_unlabel_dataset = TextDataset(unlabeled_texts, None, None, config)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, None, config)\n",
    "\n",
    "    # 初始化配置\n",
    "    config = Config()\n",
    "    config.max_length = 512\n",
    "    config.n_clusters = count_unique_labels(train_labels)\n",
    "\n",
    "    # 训练模型\n",
    "    model = train_model(train_label_dataset, train_unlabel_dataset, config)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate(model, val_dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
