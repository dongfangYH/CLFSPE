{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased', dropout=0.3):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        hidden_size = self.bert.config.hidden_size  # 移到这里获取 hidden_size\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1),  # 输出一个数值\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):  # 修改：添加 token_type_ids 参数\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)  # 修改：传递 token_type_ids\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # 取[CLS]标记对应的隐藏状态\n",
    "        regression_output = self.regressor(cls_output)\n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/story_point/mule.csv')\n",
    "data.dropna(inplace=True)\n",
    "data['text'] = data['title'] + '' + data['description']\n",
    "data['storypoint'] = data['storypoint'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方差\n",
    "data['storypoint'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['storypoint'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 5)) \n",
    "data['storypoint'].plot(kind='box')\n",
    "plt.title('Story Point')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['text'].values.tolist()\n",
    "labels = data['storypoint'].values.tolist()\n",
    "\n",
    "split_point = int(len(data) * 0.8)\n",
    "train_texts = texts[:split_point]\n",
    "train_labels = labels[:split_point]\n",
    "val_texts = texts[split_point:]\n",
    "val_labels = labels[split_point:]\n",
    "\n",
    "# 加载 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 加载 BERT 模型，用于回归任务\n",
    "model = BertForRegression()\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 创建数据集字典\n",
    "datasets = {\n",
    "    \"train\": {\"texts\": train_texts, \"labels\": train_labels},\n",
    "    \"validation\": {\"texts\": val_texts, \"labels\": val_labels}\n",
    "}\n",
    "\n",
    "# 将 tokenized_datasets 转换为 PyTorch Dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 分词\n",
    "tokenized_train = tokenize_function(train_texts)\n",
    "tokenized_val = tokenize_function(val_texts)\n",
    "\n",
    "# 数据集\n",
    "train_dataset = TextDataset(tokenized_train, train_labels)\n",
    "val_dataset = TextDataset(tokenized_val, val_labels)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 设置学习率调度器\n",
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 将模型和优化器移动到 GPU（如果可用）\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        # 将数据移到 GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = F.mse_loss(outputs, batch['labels'])\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# 在验证集上预测\n",
    "for batch in val_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'], token_type_ids=batch.get('token_type_ids'))  # 传递 token_type_ids\n",
    "    \n",
    "    predictions.extend(outputs.squeeze().cpu().numpy())  # 直接使用 outputs 作为预测结果\n",
    "    true_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "# 计算 MAE\n",
    "mae = mean_absolute_error(true_labels, predictions)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
