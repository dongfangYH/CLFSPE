{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT = 'mes_all'\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForRegression(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased', dropout=0.3):\n",
    "        super(BertForRegression, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        hidden_size = self.bert.config.hidden_size  # 移到这里获取 hidden_size\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1),  # 输出一个数值\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):  # 修改：添加 token_type_ids 参数\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)  # 修改：传递 token_type_ids\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # 取[CLS]标记对应的隐藏状态\n",
    "        regression_output = self.regressor(cls_output)\n",
    "        return regression_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 BERT 分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 加载 BERT 模型，用于回归任务\n",
    "model = BertForRegression()\n",
    "\n",
    "data = pd.read_csv(f'/kaggle/input/storypoint/{PROJECT}.csv')\n",
    "data['description'].fillna('', inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data = data[data['storypoint'] != -1]\n",
    "data['text'] = data['title'] + ' ' + data['description']\n",
    "data['label'] = data['storypoint'].astype(float)\n",
    "data = data[['text', 'label']]\n",
    "\n",
    "train_val_split_point = int(len(data) * 0.6)\n",
    "val_test_split_point = int(len(data) * 0.8)\n",
    "train_texts = data['text'][:train_val_split_point]\n",
    "train_labels = data['label'][:train_val_split_point]\n",
    "val_texts = data['text'][train_val_split_point:val_test_split_point]\n",
    "val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "test_texts = data['text'][val_test_split_point:]\n",
    "test_labels = data['label'][val_test_split_point:]\n",
    "\n",
    "# 分词函数\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 将 tokenized_datasets 转换为 PyTorch Dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 分词\n",
    "tokenized_train = tokenize_function(train_texts.to_list())\n",
    "tokenized_val = tokenize_function(val_texts.to_list())\n",
    "tokenized_test = tokenize_function(test_texts.to_list())\n",
    "\n",
    "# 数据集\n",
    "train_dataset = TextDataset(tokenized_train, train_labels)\n",
    "val_dataset = TextDataset(tokenized_val, val_labels)\n",
    "test_dataset = TextDataset(tokenized_test, test_labels)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 设置学习率调度器\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 将模型和优化器移动到 GPU（如果可用）\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 创建 TensorBoard\n",
    "writer = SummaryWriter(f'tb/{PROJECT}')\n",
    "\n",
    "# 初始化变量来保存最好的模型\n",
    "min_eval_loss_epoch = [float('inf'), 0]  # 最小验证损失和对应的 epoch\n",
    "time_records = []\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---TRAINING---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'], token_type_ids=batch.get('token_type_ids'))  \n",
    "        loss = F.mse_loss(outputs.squeeze(), batch['labels'])\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    writer.add_scalar('loss/train', avg_train_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} training completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # ---EVAL---\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'], token_type_ids=batch.get('token_type_ids'))  \n",
    "            loss = F.mse_loss(outputs.squeeze(), batch['labels'])\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    writer.add_scalar('loss/eval', avg_eval_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} evaluation completed. Avg Eval Loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "    # 保存最好的模型\n",
    "    if avg_eval_loss < min_eval_loss_epoch[0]:\n",
    "        min_eval_loss_epoch = [avg_eval_loss, epoch]\n",
    "        torch.save(model.state_dict(), f'./models/best_model_{PROJECT}.pth')  # 保存最好的模型\n",
    "\n",
    "    # ---TESTING---\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'], token_type_ids=batch.get('token_type_ids'))  \n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    # 计算 MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "    MAE_RECORDS.append(mae)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Testing MAE: {mae:.4f}\")\n",
    "\n",
    "    # 清理内存\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 删除其他模型，只保留最佳模型\n",
    "for filename in os.listdir('/kaggle/working/models'):\n",
    "    if filename.startswith('epoch') and filename != f'best_model_{PROJECT}.pth':\n",
    "        os.remove(os.path.join('/kaggle/working/models', filename))\n",
    "\n",
    "print(\"All epochs completed. Best model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
