{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref implementation\n",
    "\n",
    "https://github.com/awsm-research/gpt2sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2PreTrainedModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "BATCH_SIZE = 8\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "#DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "OUTPUT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SP(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.dense1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.dense2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "                \n",
    "        # MLP Layer\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        \n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        assert (\n",
    "            self.config.pad_token_id is not None or batch_size == 1\n",
    "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "\n",
    "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.L1Loss()\n",
    "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "def tokenization(tokenizer, text_list):\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henry\\AppData\\Local\\Temp\\ipykernel_18792\\4189838180.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['description'].fillna(' ', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/mule.csv')\n",
    "data['description'].fillna(' ', inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data['text'] = data['title'] + '' + data['description']\n",
    "data['label'] = data['storypoint'].astype(float)\n",
    "data = data[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split_point = int(len(data) * 0.6)\n",
    "val_test_split_point = int(len(data) * 0.8)\n",
    "train_text = data['text'][:train_val_split_point]\n",
    "train_labels = data['label'][:train_val_split_point]\n",
    "val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "test_text = data['text'][val_test_split_point:]\n",
    "test_labels = data['label'][val_test_split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3546, 26908, 2081, 47368, 459, 11244, 329, 1279, 439, 29, 12649, 21327, 1279, 439, 29, 7767, 6218, 4726, 3746, 13], [13921, 337, 2261, 1104, 1395, 32, 8611, 319, 1395, 32, 4133, 973, 416, 257, 8225, 2134, 7515, 5633, 1722, 345], [818, 9152, 21201, 8265, 287, 337, 2261, 4755, 6082, 10161, 346, 10115, 2727, 257, 21201, 8265, 11, 1223, 326, 338], [28446, 12, 17212, 815, 900, 6631, 21437, 351, 938, 6631, 2722, 878, 7216, 284, 23641, 48, 464, 3670, 1139, 340], [28446, 12, 17212, 815, 1104, 18305, 516, 779, 2663, 25153, 257, 2829, 18305, 516, 14626, 15741, 779, 1339, 11, 304]]\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokens_train = tokenization(tokenizer, train_text.tolist())\n",
    "tokens_val = tokenization(tokenizer, val_text.tolist())\n",
    "print(tokens_train['input_ids'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "\n",
    "tokens_test = tokenization(tokenizer, test_text.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "all_test_dataloader = []\n",
    "all_test_dataloader.append(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2SP were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "model = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      ">>> epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average training MAE loss: 8.50\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 2.46\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m avg_eval_loss, total_eval_loss\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# save model state to dict\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepo_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# testing on holdout data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\torch\\serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    944\u001b[0m         _save(\n\u001b[0;32m    945\u001b[0m             obj,\n\u001b[0;32m    946\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    949\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    950\u001b[0m         )\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\torch\\serialization.py:810\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\torch\\serialization.py:781\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    778\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, _compute_crc32)\n\u001b[0;32m    779\u001b[0m     )\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./models does not exist."
     ]
    }
   ],
   "source": [
    "# train\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "print(\"Start training ...\")\n",
    "writer = SummaryWriter('tb/mule')\n",
    "\n",
    "# vars for model selection\n",
    "min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "time_records = []\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    # ---TRAINING---\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\">>> epoch \", e)\n",
    "    # set model into train mode\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()  \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # clean memory\n",
    "        del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "    writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "    # clean memory\n",
    "    del avg_train_loss, total_train_loss\n",
    "        \n",
    "    time_records.append(time.time() - start_time)\n",
    "        \n",
    "    # ---EVAL---\n",
    "    print(\"---Evaluating ...\")\n",
    "    # set model into eval mode\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in val_dataloader:            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()  \n",
    "        # clean memory\n",
    "        del b_input_ids, b_labels, batch, result, loss, logits\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "    if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "        min_eval_loss_epoch[0] = avg_eval_loss\n",
    "        min_eval_loss_epoch[1] = e\n",
    "        \n",
    "    writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "    # clean memory\n",
    "    del avg_eval_loss, total_eval_loss\n",
    "    # save model state to dict\n",
    "    torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "    print(\"===============================\")\n",
    "        \n",
    "    # testing on holdout data\n",
    "    index = 0\n",
    "    for test_dataloader in all_test_dataloader:\n",
    "        index += 1\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "            b_input_ids, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids)\n",
    "            logits = logits['logits'].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "        # calculate errors\n",
    "        distance_records = []\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(len(predictions[i])):\n",
    "                distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                distance_records.append(distance)\n",
    "\n",
    "        ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "        MAE = np.mean(np.array(distance_records)) \n",
    "        ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "        MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "        MAE_RECORDS.append(MAE)\n",
    "        MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "        OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "        OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "        OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "        print('MAE: ', MAE)\n",
    "        print('MdAE: ', MdAE)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "    \n",
    "# select model\n",
    "os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "            'models/mule_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "# del unwanted models\n",
    "for i in range(20):\n",
    "    try:\n",
    "        os.remove(\"models/epo_\" + str(i))\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "        + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "print('all done for one project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
