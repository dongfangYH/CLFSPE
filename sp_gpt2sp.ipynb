{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref implementation\n",
    "\n",
    "https://github.com/awsm-research/gpt2sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2PreTrainedModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import time\n",
    "from transformers import GPT2Config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "OUTPUT = ''\n",
    "PROJECT = 'mes_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SP(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.dense1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.dense2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "                \n",
    "        # MLP Layer\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        \n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        assert (\n",
    "            self.config.pad_token_id is not None or batch_size == 1\n",
    "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "\n",
    "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.L1Loss()\n",
    "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "def tokenization(tokenizer, text_list):\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'/kaggle/input/storypoint/{PROJECT}.csv')\n",
    "data = data[data['storypoint'] != -1]\n",
    "data['description'] = data['description'].fillna('')\n",
    "data.dropna(inplace=True)\n",
    "data['text'] = data['title'] + ' ' + data['description']\n",
    "data['label'] = data['storypoint'].astype(float)\n",
    "data = data[['text', 'label']]\n",
    "\n",
    "train_val_split_point = int(len(data) * 0.6)\n",
    "val_test_split_point = int(len(data) * 0.8)\n",
    "train_text = data['text'][:train_val_split_point]\n",
    "train_labels = data['label'][:train_val_split_point]\n",
    "val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "test_text = data['text'][val_test_split_point:]\n",
    "test_labels = data['label'][val_test_split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokens_train = tokenization(tokenizer, train_text.tolist())\n",
    "tokens_val = tokenization(tokenizer, val_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "\n",
    "tokens_test = tokenization(tokenizer, test_text.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "all_test_dataloader = []\n",
    "all_test_dataloader.append(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "model = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "print(\"Start training ...\")\n",
    "writer = SummaryWriter(f'/kaggle/working/tb/{PROJECT}')\n",
    "\n",
    "# vars for model selection\n",
    "min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "time_records = []\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    # ---TRAINING---\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\">>> epoch \", e)\n",
    "    # set model into train mode\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()  \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # clean memory\n",
    "        del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "    writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "    # clean memory\n",
    "    del avg_train_loss, total_train_loss\n",
    "        \n",
    "    time_records.append(time.time() - start_time)\n",
    "        \n",
    "    # ---EVAL---\n",
    "    print(\"---Evaluating ...\")\n",
    "    # set model into eval mode\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in val_dataloader:            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()  \n",
    "        # clean memory\n",
    "        del b_input_ids, b_labels, batch, result, loss, logits\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "    if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "        min_eval_loss_epoch[0] = avg_eval_loss\n",
    "        min_eval_loss_epoch[1] = e\n",
    "        \n",
    "    writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "    # clean memory\n",
    "    del avg_eval_loss, total_eval_loss\n",
    "    # save model state to dict\n",
    "    torch.save(model.state_dict(), '/kaggle/working/models/' + 'epo_' + str(e))\n",
    "        \n",
    "    print(\"===============================\")\n",
    "        \n",
    "    # testing on holdout data\n",
    "    index = 0\n",
    "    for test_dataloader in all_test_dataloader:\n",
    "        index += 1\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "            b_input_ids, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids)\n",
    "            logits = logits['logits'].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "        # calculate errors\n",
    "        distance_records = []\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(len(predictions[i])):\n",
    "                distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                distance_records.append(distance)\n",
    "\n",
    "        ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "        MAE = np.mean(np.array(distance_records)) \n",
    "        ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "        MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "        MAE_RECORDS.append(MAE)\n",
    "        MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "        OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "        OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "        OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "        print('MAE: ', MAE)\n",
    "        print('MdAE: ', MdAE)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "    \n",
    "# select model\n",
    "os.rename('/kaggle/working/models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "            f'/kaggle/working/models/{PROJECT}_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "# del unwanted models\n",
    "for i in range(20):\n",
    "    try:\n",
    "        os.remove(\"/kaggle/working/models/epo_\" + str(i))\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "        + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "print('all done for one project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
