{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref implementation\n",
    "\n",
    "https://github.com/awsm-research/gpt2sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, GPT2PreTrainedModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import GPT2Config\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "BATCH_SIZE = 8\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "#DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "OUTPUT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SP(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.dense1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.dense2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.score = nn.Linear(config.n_embd, self.num_labels, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "                \n",
    "        # MLP Layer\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        \n",
    "        logits = self.score(hidden_states)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size, sequence_length = input_ids.shape[:2]\n",
    "        else:\n",
    "            batch_size, sequence_length = inputs_embeds.shape[:2]\n",
    "\n",
    "        assert (\n",
    "            self.config.pad_token_id is not None or batch_size == 1\n",
    "        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "\n",
    "        pooled_logits = logits[range(batch_size), sequence_lengths]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = nn.L1Loss()\n",
    "                loss = loss_fct(pooled_logits.view(-1), labels.to(self.dtype).view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "def tokenization(tokenizer, text_list):\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/mes_all.csv')\n",
    "data.drop_duplicates(inplace=True)\n",
    "data['description'] = data['description'].fillna('')\n",
    "data.dropna(inplace=True)\n",
    "data = data[data['storypoint'] != -1]\n",
    "data['text'] = data['title'] # + ' ' + data['description']\n",
    "data['label'] = data['storypoint'].astype(float)\n",
    "data = data[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split_point = int(len(data) * 0.6)\n",
    "val_test_split_point = int(len(data) * 0.8)\n",
    "train_text = data['text'][:train_val_split_point]\n",
    "train_labels = data['label'][:train_val_split_point]\n",
    "val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "test_text = data['text'][val_test_split_point:]\n",
    "test_labels = data['label'][val_test_split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3791, 10959, 3188, 410, 16, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [34500, 8821, 262, 2166, 437, 636, 329, 262, 1312, 1507, 77, 2139, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [11712, 554, 532, 28062, 515, 2836, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [1890, 23442, 9206, 12, 1729, 12, 69, 5702, 515, 2985, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256], [38263, 17641, 2792, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokens_train = tokenization(tokenizer, train_text.tolist())\n",
    "tokens_val = tokenization(tokenizer, val_text.tolist())\n",
    "# print(tokens_train['input_ids'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "\n",
    "tokens_test = tokenization(tokenizer, test_text.tolist())\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "all_test_dataloader = []\n",
    "all_test_dataloader.append(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2SP were not initialized from the model checkpoint at gpt2 and are newly initialized: ['dense1.weight', 'dense2.weight', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config(num_labels=1, pad_token_id=50256)\n",
    "model = GPT2SP.from_pretrained('gpt2', config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 5.21\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.55\n",
      "===============================\n",
      "MAE:  1.4354827\n",
      "MdAE:  0.5612238\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.72\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 2.14\n",
      "===============================\n",
      "MAE:  2.2115936\n",
      "MdAE:  2.0872643\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.58\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.42\n",
      "===============================\n",
      "MAE:  1.3985846\n",
      "MdAE:  1.0143205\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.37\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.44\n",
      "===============================\n",
      "MAE:  1.4866471\n",
      "MdAE:  1.000987\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.18\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.48\n",
      "===============================\n",
      "MAE:  1.630568\n",
      "MdAE:  1.2613665\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.01\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.71\n",
      "===============================\n",
      "MAE:  1.9307662\n",
      "MdAE:  1.6310644\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.91\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.59\n",
      "===============================\n",
      "MAE:  1.6987953\n",
      "MdAE:  1.2981381\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.85\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.53\n",
      "===============================\n",
      "MAE:  1.678851\n",
      "MdAE:  1.3127961\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.70\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.47\n",
      "===============================\n",
      "MAE:  1.5470262\n",
      "MdAE:  1.1081216\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.65\n",
      "---Evaluating ...\n",
      " Average eval MAE loss: 1.48\n",
      "===============================\n",
      "MAE:  1.610185\n",
      "MdAE:  1.1651871\n",
      "all done for one project\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "print(\"Start training ...\")\n",
    "writer = SummaryWriter('tb/mes_all')\n",
    "\n",
    "# vars for model selection\n",
    "min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "time_records = []\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    # ---TRAINING---\n",
    "    # clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\">>> epoch \", e)\n",
    "    # set model into train mode\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_train_loss += loss.item()  \n",
    "        loss.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # clean memory\n",
    "        del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "    writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "    # clean memory\n",
    "    del avg_train_loss, total_train_loss\n",
    "        \n",
    "    time_records.append(time.time() - start_time)\n",
    "        \n",
    "    # ---EVAL---\n",
    "    print(\"---Evaluating ...\")\n",
    "    # set model into eval mode\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    for batch in val_dataloader:            \n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_labels = batch[1].to(DEVICE)\n",
    "        model.zero_grad()\n",
    "        result = model(b_input_ids, \n",
    "                        labels=b_labels,\n",
    "                        return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "        total_eval_loss += loss.item()  \n",
    "        # clean memory\n",
    "        del b_input_ids, b_labels, batch, result, loss, logits\n",
    "    avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "    if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "        min_eval_loss_epoch[0] = avg_eval_loss\n",
    "        min_eval_loss_epoch[1] = e\n",
    "        \n",
    "    writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "    # clean memory\n",
    "    del avg_eval_loss, total_eval_loss\n",
    "    # save model state to dict\n",
    "    torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "    print(\"===============================\")\n",
    "        \n",
    "    # testing on holdout data\n",
    "    index = 0\n",
    "    for test_dataloader in all_test_dataloader:\n",
    "        index += 1\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "            b_input_ids, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids)\n",
    "            logits = logits['logits'].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "        # calculate errors\n",
    "        distance_records = []\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(len(predictions[i])):\n",
    "                distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                distance_records.append(distance)\n",
    "\n",
    "        ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "        MAE = np.mean(np.array(distance_records)) \n",
    "        ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "        MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "        MAE_RECORDS.append(MAE)\n",
    "        MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "        OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "        OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "        OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "        print('MAE: ', MAE)\n",
    "        print('MdAE: ', MdAE)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "    \n",
    "# select model\n",
    "os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "            'models/mule_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "# del unwanted models\n",
    "for i in range(20):\n",
    "    try:\n",
    "        os.remove(\"models/epo_\" + str(i))\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "        + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "print('all done for one project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
