{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref simsiam implementation\n",
    "\n",
    "https://github.com/facebookresearch/simsiam/blob/main/main_simsiam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from RandomCompositeTransformation import RandomCompositeTransformation as CompositeAugmenter\n",
    "import time\n",
    "import random\n",
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapWordNet,WordSwapEmbedding, WordInnerSwapRandom, WordDeletion, BackTranslation, WordSwapExtend, WordSwapRandomCharacterSubstitution, WordSwapHomoglyphSwap, WordSwapRandomCharacterInsertion\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "TRAIN_EPOCH = 10\n",
    "FINE_TUNE_EPOCH = 5\n",
    "TRAIN_BATCH = 16\n",
    "FINE_TUNE_BATCH = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "PROJECT = 'mes_all'\n",
    "SAVE_PATH = f'/kaggle/working/models/best_model_{PROJECT}.pth'\n",
    "\n",
    "PRINT_TEXT_PROCESS_TIME = True\n",
    "AUGMENT_TEXT_LEN = 200\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading text data with optional augmentation.\"\"\"\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128, augment=False, augmenter=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.augmenter = augmenter               \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            augmented_texts = self.text_augment(text)\n",
    "            text_view1 = augmented_texts[0]\n",
    "            text_view2 = augmented_texts[1]\n",
    "        else:\n",
    "            text_view1 = text\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text_view1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs1 = {key: val.squeeze(0) for key, val in inputs1.items()}\n",
    "\n",
    "        if self.augment:\n",
    "            inputs2 = self.tokenizer(\n",
    "                text_view2,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs2 = {key: val.squeeze(0) for key, val in inputs2.items()}\n",
    "            return inputs1, inputs2\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return inputs1, label\n",
    "        return inputs1\n",
    "\n",
    "    def text_augment(self, text):\n",
    "        \"\"\"text augmentation.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if len(text) > AUGMENT_TEXT_LEN:\n",
    "            text_to_augment = text[:AUGMENT_TEXT_LEN]\n",
    "            remaining_text = text[AUGMENT_TEXT_LEN:]\n",
    "        else:\n",
    "            text_to_augment = text\n",
    "            remaining_text = ''\n",
    "        \n",
    "        augmented_texts = self.augmenter.augment(text_to_augment)\n",
    "        while True:\n",
    "            if augmented_texts[0] != augmented_texts[1]:\n",
    "                break\n",
    "            print('augmented_text1 equals to augmented_text2', augmented_texts[0])\n",
    "            augmented_texts = self.augmenter.augment(text)\n",
    "        spent = time.time() - start_time\n",
    "        if PRINT_TEXT_PROCESS_TIME:\n",
    "            print('process text cost', spent, 'seconds')\n",
    "        return (augmented_texts[0] + remaining_text, augmented_texts[1] + remaining_text)\n",
    "\n",
    "class SimSiamText(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
    "        super(SimSiamText, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=True),\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(dim, pred_dim, bias=False),\n",
    "            nn.BatchNorm1d(pred_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pred_dim, dim, bias=True),\n",
    "        )\n",
    "        self.regressor = nn.Linear(768, 1)  # 线性回归头\n",
    "\n",
    "    def forward(self, x1, x2=None, regression=False):\n",
    "        \"\"\"\n",
    "          定义模型的前向传播过程\n",
    "          x1 和 x2 是同一批图像的两个不同增强视图。\n",
    "          z1 和 z2 是编码器对 x1 和 x2 的编码结果。\n",
    "          p1 和 p2 是预测器对 z1 和 z2 的预测结果。\n",
    "        \"\"\"\n",
    "        if regression:\n",
    "            return self.regressor(self.encoder(**x1)[\"pooler_output\"])  # 仅用于回归\n",
    "        z1 = self.projector(self.encoder(**x1)[\"pooler_output\"])\n",
    "        z2 = self.projector(self.encoder(**x2)[\"pooler_output\"])\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        # z1.detach() 和 z2.detach() 表示在反向传播时不计算 z1 和 z2 的梯度，因为它们只作为目标使用。\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "    \n",
    "    def remove_projection_head(self):\n",
    "        \"\"\"移除 projector 和 predictor 用于微调\"\"\"\n",
    "        self.projector = None\n",
    "        self.predictor = None\n",
    "\n",
    "\n",
    "def train_simsiam(model, dataloader, criterion, optimizer, scheduler, device, print_freq=10):\n",
    "    \"\"\"训练 SimSiam 并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(TRAIN_EPOCH):  # 预训练\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs1 = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            inputs2 = {key: val.to(device) for key, val in batch[1].items()}\n",
    "\n",
    "            p1, p2, z1, z2 = model(inputs1, inputs2)\n",
    "            loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{TRAIN_EPOCH}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{TRAIN_EPOCH}] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "        # 保存最好的模型\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), SAVE_PATH)  # 保存当前最好的模型\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "def fine_tune_and_eval(model, labeled_data, tokenizer, criterion, optimizer, scheduler, device, print_freq=10):\n",
    "    \"\"\"微调 SimSiam 进行回归任务，并打印训练进度和损失\"\"\"\n",
    "    split_idx = int(len(labeled_data) * 0.8)\n",
    "\n",
    "    fine_tune_data = labeled_data.iloc[:split_idx]\n",
    "\n",
    "    fine_tune_texts = fine_tune_data['text'].values.tolist()\n",
    "    fine_tune_labels = fine_tune_data['storypoint'].values.tolist()\n",
    "\n",
    "    # 微调（回归任务）\n",
    "    labeled_dataset = TextDataset(fine_tune_texts, fine_tune_labels, tokenizer=tokenizer, max_length=MAX_LEN, augment=False)\n",
    "    labeled_dataloader = DataLoader(labeled_dataset, batch_size=FINE_TUNE_BATCH, shuffle=True)\n",
    "\n",
    "    test_data = labeled_data.iloc[split_idx:]\n",
    "    test_texts = test_data['text'].values.tolist()\n",
    "    test_labels = test_data['storypoint'].values.tolist()\n",
    "\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer=tokenizer, max_length=MAX_LEN, augment=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(FINE_TUNE_EPOCH):  # 微调\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(labeled_dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device).float()\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()  # 取回归输出\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Fine-tune Epoch [{epoch+1}/{FINE_TUNE_EPOCH}], Step [{batch_idx}/{len(labeled_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(labeled_dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Fine-tune Epoch [{epoch+1}/{FINE_TUNE_EPOCH}] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "        # 评估模型\n",
    "        evaluate(model, test_dataloader, device)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"评估模型在测试集上的 MAE 误差\"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()\n",
    "            mae = torch.abs(preds - labels).sum().item()\n",
    "            total_mae += mae\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "    avg_mae = total_mae / num_samples\n",
    "    print(f\"Evaluation - MAE: {avg_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# 定义不同的增强方法\n",
    "#wordnet_transformation = WordSwapWordNet()\n",
    "embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "#backtranslate_transformation = BackTranslation(chained_back_translation=2)\n",
    "extendword_transformation = WordSwapExtend()\n",
    "randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "# 组合多个增强方法，并指定执行概率\n",
    "random_composite_transformation = CompositeAugmenter(\n",
    "    transformations=[\n",
    "        extendword_transformation, \n",
    "        homoglyphswap_transformation, \n",
    "        embedding_transformation, \n",
    "        randomwordsubs_transformation,\n",
    "        randomcharinsert_transformation\n",
    "    ],\n",
    "    probabilities=[1, 0.5, 0.5, 0.2, 0.2]  # 执行概率\n",
    ")\n",
    "\n",
    "# 定义约束，避免对停用词进行修改，防止重复修改\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "# 语义相似性约束\n",
    "# semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "# constraints.append(semantic_constraint)\n",
    "\n",
    "# 创建增强器\n",
    "text_augmenter = Augmenter(\n",
    "    transformation=random_composite_transformation,\n",
    "    constraints=constraints,\n",
    "    pct_words_to_swap=0.1,\n",
    "    transformations_per_example=2  # 生成2个不同版本的增强文本\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv(f'/kaggle/input/storypoint/{PROJECT}.csv')\n",
    "data = data.drop_duplicates(subset='issuekey', keep='first')\n",
    "data['description'] = data['description'].fillna('')\n",
    "data.dropna(inplace=True)\n",
    "data['text'] = data['title'] + ' ' + data['description']\n",
    "texts = data['text'].values.tolist()\n",
    "\n",
    "# 预训练数据集\n",
    "dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=MAX_LEN, augmenter=text_augmenter)\n",
    "dataloader = DataLoader(dataset, batch_size=TRAIN_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/146], Loss: -0.0018\n",
      "Epoch [1/10], Step [10/146], Loss: -0.1428\n",
      "Epoch [1/10], Step [20/146], Loss: -0.2051\n",
      "Epoch [1/10], Step [30/146], Loss: -0.3784\n",
      "Epoch [1/10], Step [40/146], Loss: -0.4995\n",
      "Epoch [1/10], Step [50/146], Loss: -0.6043\n",
      "Epoch [1/10], Step [60/146], Loss: -0.6651\n",
      "Epoch [1/10], Step [70/146], Loss: -0.6484\n",
      "Epoch [1/10], Step [80/146], Loss: -0.7270\n",
      "Epoch [1/10], Step [90/146], Loss: -0.7281\n",
      "Epoch [1/10], Step [100/146], Loss: -0.7923\n",
      "Epoch [1/10], Step [110/146], Loss: -0.8425\n",
      "Epoch [1/10], Step [120/146], Loss: -0.8417\n",
      "Epoch [1/10], Step [130/146], Loss: -0.8392\n",
      "Epoch [1/10], Step [140/146], Loss: -0.8538\n",
      "Epoch [1/10] Completed | Avg Loss: -0.6105 | Time: 347.76s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "# 预训练 SimSiam\n",
    "criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "#optimizer = torch.optim.Adam(simsiam.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.SGD(simsiam.parameters(), LEARNING_RATE, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.7)  # 每3个epoch，学习率衰减为原来的0.7\n",
    "train_simsiam(simsiam, dataloader, criterion, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# 加载最优模型并进行微调\n",
    "simsiam.load_state_dict(torch.load(SAVE_PATH))  # 加载最佳模型\n",
    "simsiam = simsiam.to(device)\n",
    "\n",
    "# 移除 projector 和 predictor\n",
    "simsiam.remove_projection_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "labeled_data = data[data['storypoint'] != -1]\n",
    "regression_criterion = nn.MSELoss().to(device)\n",
    "\n",
    "# 微调并测试\n",
    "fine_tune_and_eval(simsiam, labeled_data, tokenizer, regression_criterion, optimizer, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
