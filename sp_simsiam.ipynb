{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref simsiam implementation\n",
    "\n",
    "https://github.com/facebookresearch/simsiam/blob/main/main_simsiam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from RandomCompositeTransformation import RandomCompositeTransformation as CompositeAugmenter\n",
    "import time\n",
    "import random\n",
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapWordNet, WordInnerSwapRandom, WordDeletion, BackTranslation, WordSwapExtend, WordSwapRandomCharacterSubstitution, WordSwapHomoglyphSwap, WordSwapRandomCharacterInsertion\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading text data with optional augmentation.\"\"\"\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128, augment=False, augmenter=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.augmenter = augmenter\n",
    "\n",
    "        if self.augment:\n",
    "            self.text_views1 = []\n",
    "            self.text_views2 = []\n",
    "            for text in self.texts:\n",
    "                # Generate two augmented views for each text\n",
    "                augmented_text1 = self.text_augment(text)\n",
    "\n",
    "                while True:\n",
    "                    augmented_text2 = self.text_augment(text)\n",
    "                    if augmented_text1 != augmented_text2:\n",
    "                        break\n",
    "                    print('augmented_text1 equals to augmented_text2', augmented_text1)\n",
    "\n",
    "                self.text_views1.append(augmented_text1)\n",
    "                self.text_views2.append(augmented_text2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            text_view1 = self.text_views1[idx]\n",
    "            text_view2 = self.text_views2[idx]\n",
    "        else:\n",
    "            text_view1 = text\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text_view1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs1 = {key: val.squeeze(0) for key, val in inputs1.items()}\n",
    "\n",
    "        if self.augment:\n",
    "            inputs2 = self.tokenizer(\n",
    "                text_view2,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs2 = {key: val.squeeze(0) for key, val in inputs2.items()}\n",
    "            return inputs1, inputs2\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return inputs1, label\n",
    "        return inputs1\n",
    "\n",
    "    def text_augment(self, text):\n",
    "        \"\"\"text augmentation.\"\"\"\n",
    "        augmented_texts = self.augmenter.augment(text)\n",
    "        return random.choice(augmented_texts)\n",
    "\n",
    "class SimSiamText(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
    "        super(SimSiamText, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=True),\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(dim, pred_dim, bias=False),\n",
    "            nn.BatchNorm1d(pred_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pred_dim, dim, bias=True),\n",
    "        )\n",
    "        self.regressor = nn.Linear(768, 1)  # 线性回归头\n",
    "\n",
    "    def forward(self, x1, x2=None, regression=False):\n",
    "        \"\"\"\n",
    "          定义模型的前向传播过程\n",
    "          x1 和 x2 是同一批图像的两个不同增强视图。\n",
    "          z1 和 z2 是编码器对 x1 和 x2 的编码结果。\n",
    "          p1 和 p2 是预测器对 z1 和 z2 的预测结果。\n",
    "        \"\"\"\n",
    "        if regression:\n",
    "            return self.regressor(self.encoder(**x1)[\"pooler_output\"])  # 仅用于回归\n",
    "        z1 = self.projector(self.encoder(**x1)[\"pooler_output\"])\n",
    "        z2 = self.projector(self.encoder(**x2)[\"pooler_output\"])\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        # z1.detach() 和 z2.detach() 表示在反向传播时不计算 z1 和 z2 的梯度，因为它们只作为目标使用。\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "    \n",
    "    def remove_projection_head(self):\n",
    "        \"\"\"移除 projector 和 predictor 以用于微调\"\"\"\n",
    "        self.projector = None\n",
    "        self.predictor = None\n",
    "\n",
    "\n",
    "def train_simsiam(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"训练 SimSiam 并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(10):  # 预训练 10 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs1 = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            inputs2 = {key: val.to(device) for key, val in batch[1].items()}\n",
    "\n",
    "            p1, p2, z1, z2 = model(inputs1, inputs2)\n",
    "            loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/10], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/10] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def fine_tune(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"微调 SimSiam 进行回归任务，并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(5):  # 微调 5 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device).float()\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()  # 取回归输出\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Fine-tune Epoch [{epoch+1}/5], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Fine-tune Epoch [{epoch+1}/5] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"评估模型在测试集上的 MAE 误差\"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()\n",
    "            mae = torch.abs(preds - labels).sum().item()  # 计算 MAE\n",
    "            total_mae += mae\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "    avg_mae = total_mae / num_samples\n",
    "    print(f\"Evaluation - MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# 主运行逻辑\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # 定义不同的增强方法\n",
    "    wordnet_transformation = WordSwapWordNet()\n",
    "    #embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "    backtranslate_transformation = BackTranslation(chained_back_translation=2)\n",
    "    #extendword_transformation = WordSwapExtend()\n",
    "    randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "    homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "    randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "    # 组合多个增强方法，并指定执行概率\n",
    "    random_composite_transformation = CompositeAugmenter(\n",
    "        transformations=[\n",
    "            backtranslate_transformation, \n",
    "            homoglyphswap_transformation, \n",
    "            wordnet_transformation, \n",
    "            randomwordsubs_transformation,\n",
    "            randomcharinsert_transformation\n",
    "        ],\n",
    "        probabilities=[1, 0.5, 0.5, 0.1, 0.1]  # 执行概率\n",
    "    )\n",
    "\n",
    "    # 定义约束，避免对停用词进行修改，防止重复修改\n",
    "    constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "    # 语义相似性约束\n",
    "    semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "    constraints.append(semantic_constraint)\n",
    "\n",
    "    # 创建增强器\n",
    "    text_augmenter = Augmenter(\n",
    "        transformation=random_composite_transformation,\n",
    "        constraints=constraints,\n",
    "        pct_words_to_swap=0.1,\n",
    "        transformations_per_example=3  # 生成 3 个不同版本的增强文本\n",
    "    )\n",
    "\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('data/mes_all.csv')\n",
    "    data['text'] = data['title'] + ' ' + data['description']\n",
    "    texts = data['text'].value.to_list()\n",
    "\n",
    "    # 预训练数据集\n",
    "    dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=512, augmenter=text_augmenter)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "    # 预训练 SimSiam\n",
    "    criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    train_simsiam(simsiam, dataloader, criterion, optimizer, device)\n",
    "\n",
    "    # **移除 projector 和 predictor**\n",
    "    simsiam.remove_projection_head()\n",
    "\n",
    "    # 划分数据集\n",
    "    labeled_data = data[data['storypoint'] != -1]\n",
    "    split_idx = int(len(labeled_data) * 0.8)\n",
    "\n",
    "    fine_tune_data = labeled_data.iloc[:split_idx]\n",
    "\n",
    "    fine_tune_texts = fine_tune_data['text'].values.to_list()\n",
    "    fine_tune_labels = fine_tune_data['storypoint'].values.to_list()\n",
    "\n",
    "    # 微调（回归任务）\n",
    "    labeled_dataset = TextDataset(fine_tune_texts, fine_tune_labels, tokenizer=tokenizer, augment=False)\n",
    "    labeled_dataloader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    regression_criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    fine_tune(simsiam, labeled_dataloader, regression_criterion, optimizer, device)\n",
    "\n",
    "    test_data = labeled_data.iloc[split_idx:]\n",
    "    test_texts = test_data['text'].values.to_list()\n",
    "    test_labels = test_data['storypoint'].values.to_list()\n",
    "\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer=tokenizer, augment=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate(simsiam, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# 定义不同的增强方法\n",
    "wordnet_transformation = WordSwapWordNet()\n",
    "worddelete_transformation = WordDeletion()\n",
    "wordinnerswap_transformation = WordInnerSwapRandom()\n",
    "#extendword_transformation = WordSwapExtend()\n",
    "randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "#homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "\n",
    "# 组合多个增强方法，并指定执行概率\n",
    "random_composite_transformation = CompositeAugmenter(\n",
    "    transformations=[\n",
    "        worddelete_transformation, \n",
    "        wordinnerswap_transformation, \n",
    "        wordnet_transformation, \n",
    "        randomwordsubs_transformation,\n",
    "        randomcharinsert_transformation\n",
    "    ],\n",
    "    probabilities=[0.1, 0.5, 0.5, 0.1, 0.1]  # 执行概率\n",
    ")\n",
    "\n",
    "# 定义约束，避免对停用词进行修改，防止重复修改\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "# 语义相似性约束\n",
    "semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "constraints.append(semantic_constraint)\n",
    "\n",
    "# 创建增强器\n",
    "text_augmenter = Augmenter(\n",
    "    transformation=random_composite_transformation,\n",
    "    constraints=constraints,\n",
    "    pct_words_to_swap=0.1,\n",
    "    transformations_per_example=2  # 生成 2 个不同版本的增强文本\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv('./data/bamboo.csv')\n",
    "data.dropna(inplace=True)\n",
    "data['text'] = data['title'] + ' ' + data['description']\n",
    "texts = data['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 预训练数据集\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_augmenter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, texts, labels, tokenizer, max_length, augment, augmenter)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_views2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Generate two augmented views for each text\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     augmented_text1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_augment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         augmented_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_augment(text)\n",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m, in \u001b[0;36mTextDataset.text_augment\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_augment\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"text augmentation.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     augmented_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(augmented_texts)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\augmentation\\augmenter.py:126\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    123\u001b[0m words_swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m words_swapped \u001b[38;5;241m<\u001b[39m num_words_to_swap:\n\u001b[1;32m--> 126\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# Get rid of transformations we already have\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    132\u001b[0m         t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transformed_texts \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_transformed_texts\n\u001b[0;32m    133\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\transformations\\composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m new_attacked_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[1;32m---> 39\u001b[0m     new_attacked_texts\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\transformations\\transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[1;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[1;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[0;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\transformations\\word_swaps\\word_swap.py:52\u001b[0m, in \u001b[0;36mWordSwap._get_transformations\u001b[1;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m word_to_replace:\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m         transformed_texts_idx\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_word_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m     transformed_texts\u001b[38;5;241m.\u001b[39mextend(transformed_texts_idx)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\shared\\attacked_text.py:359\u001b[0m, in \u001b[0;36mAttackedText.replace_word_at_index\u001b[1;34m(self, index, new_word)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_word, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace_word_at_index requires ``str`` new_word, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_word)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m     )\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_words_at_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_word\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\shared\\attacked_text.py:350\u001b[0m, in \u001b[0;36mAttackedText.replace_words_at_indices\u001b[1;34m(self, indices, new_words)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign word at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    349\u001b[0m     words[i] \u001b[38;5;241m=\u001b[39m new_word\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_new_attacked_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\shared\\attacked_text.py:421\u001b[0m, in \u001b[0;36mAttackedText.generate_new_attacked_text\u001b[1;34m(self, new_words)\u001b[0m\n\u001b[0;32m    419\u001b[0m perturbed_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_text[:word_start]\n\u001b[0;32m    420\u001b[0m original_text \u001b[38;5;241m=\u001b[39m original_text[word_end:]\n\u001b[1;32m--> 421\u001b[0m adv_words \u001b[38;5;241m=\u001b[39m \u001b[43mwords_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43madv_word_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m adv_num_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(adv_words)\n\u001b[0;32m    423\u001b[0m num_words_diff \u001b[38;5;241m=\u001b[39m adv_num_words \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(words_from_text(input_word))\n",
      "File \u001b[1;32mc:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\textattack\\shared\\utils\\strings.py:55\u001b[0m, in \u001b[0;36mwords_from_text\u001b[1;34m(s, words_to_ignore)\u001b[0m\n\u001b[0;32m     53\u001b[0m     filt \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mlstrip(exceptions) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(filter_pattern, word)]\n\u001b[0;32m     54\u001b[0m     words\u001b[38;5;241m.\u001b[39mextend(filt)\n\u001b[1;32m---> 55\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords_to_ignore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 预训练数据集\n",
    "dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=256, augmenter=text_augmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "# 预训练 SimSiam\n",
    "criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "train_simsiam(simsiam, dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除 projector 和 predictor\n",
    "simsiam.remove_projection_head()\n",
    "\n",
    "# 划分数据集\n",
    "labeled_data = data[data['storypoint'] != -1]\n",
    "split_idx = int(len(labeled_data) * 0.8)\n",
    "\n",
    "fine_tune_data = labeled_data.iloc[:split_idx]\n",
    "\n",
    "fine_tune_texts = fine_tune_data['text'].values.tolist()\n",
    "fine_tune_labels = fine_tune_data['storypoint'].values.tolist()\n",
    "\n",
    "# 微调（回归任务）\n",
    "labeled_dataset = TextDataset(fine_tune_texts, fine_tune_labels, tokenizer=tokenizer, augment=False)\n",
    "labeled_dataloader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "regression_criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "fine_tune(simsiam, labeled_dataloader, regression_criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = labeled_data.iloc[split_idx:]\n",
    "test_texts = test_data['text'].values.tolist()\n",
    "test_labels = test_data['storypoint'].values.tolist()\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer=tokenizer, augment=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 评估模型\n",
    "evaluate(simsiam, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
