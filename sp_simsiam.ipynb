{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref simsiam implementation\n",
    "\n",
    "https://github.com/facebookresearch/simsiam/blob/main/main_simsiam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from RandomCompositeTransformation import RandomCompositeTransformation as CompositeAugmenter\n",
    "import time\n",
    "import random\n",
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapWordNet, WordSwapEmbedding, BackTranslation, WordSwapExtend, WordSwapRandomCharacterSubstitution, WordSwapHomoglyphSwap, WordSwapRandomCharacterInsertion\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading text data with optional augmentation.\"\"\"\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128, augment=False, augmenter=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.augmenter = augmenter\n",
    "\n",
    "        if self.augment:\n",
    "            self.text_views1 = []\n",
    "            self.text_views2 = []\n",
    "            for text in self.texts:\n",
    "                # Generate two augmented views for each text\n",
    "                augmented_text1 = self.text_augment(text)\n",
    "\n",
    "                while True:\n",
    "                    augmented_text2 = self.text_augment(text)\n",
    "                    if augmented_text1 != augmented_text2:\n",
    "                        break\n",
    "                    print('augmented_text1 equals to augmented_text2', augmented_text1)\n",
    "\n",
    "                self.text_views1.append(augmented_text1)\n",
    "                self.text_views2.append(augmented_text2)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            text_view1 = self.text_views1[idx]\n",
    "            text_view2 = self.text_views2[idx]\n",
    "        else:\n",
    "            text_view1 = text\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text_view1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs1 = {key: val.squeeze(0) for key, val in inputs1.items()}\n",
    "\n",
    "        if self.augment:\n",
    "            inputs2 = self.tokenizer(\n",
    "                text_view2,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs2 = {key: val.squeeze(0) for key, val in inputs2.items()}\n",
    "            return inputs1, inputs2\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return inputs1, label\n",
    "        return inputs1\n",
    "\n",
    "    def text_augment(self, text):\n",
    "        \"\"\"text augmentation.\"\"\"\n",
    "        augmented_texts = self.augmenter.augment(text)\n",
    "        return random.choice(augmented_texts)\n",
    "\n",
    "class SimSiamText(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
    "        super(SimSiamText, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=True),\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(dim, pred_dim, bias=False),\n",
    "            nn.BatchNorm1d(pred_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pred_dim, dim, bias=True),\n",
    "        )\n",
    "        self.regressor = nn.Linear(768, 1)  # 线性回归头\n",
    "\n",
    "    def forward(self, x1, x2=None, regression=False):\n",
    "        \"\"\"\n",
    "          定义模型的前向传播过程\n",
    "          x1 和 x2 是同一批图像的两个不同增强视图。\n",
    "          z1 和 z2 是编码器对 x1 和 x2 的编码结果。\n",
    "          p1 和 p2 是预测器对 z1 和 z2 的预测结果。\n",
    "        \"\"\"\n",
    "        if regression:\n",
    "            return self.regressor(self.encoder(**x1)[\"pooler_output\"])  # 仅用于回归\n",
    "        z1 = self.projector(self.encoder(**x1)[\"pooler_output\"])\n",
    "        z2 = self.projector(self.encoder(**x2)[\"pooler_output\"])\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        # z1.detach() 和 z2.detach() 表示在反向传播时不计算 z1 和 z2 的梯度，因为它们只作为目标使用。\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "    \n",
    "    def remove_projection_head(self):\n",
    "        \"\"\"移除 projector 和 predictor 以用于微调\"\"\"\n",
    "        self.projector = None\n",
    "        self.predictor = None\n",
    "\n",
    "\n",
    "def train_simsiam(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"训练 SimSiam 并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(10):  # 预训练 10 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs1 = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            inputs2 = {key: val.to(device) for key, val in batch[1].items()}\n",
    "\n",
    "            p1, p2, z1, z2 = model(inputs1, inputs2)\n",
    "            loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/10], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/10] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def fine_tune(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"微调 SimSiam 进行回归任务，并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(5):  # 微调 5 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device).float()\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()  # 取回归输出\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Fine-tune Epoch [{epoch+1}/5], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Fine-tune Epoch [{epoch+1}/5] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"评估模型在测试集上的 MAE 误差\"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()\n",
    "            mae = torch.abs(preds - labels).sum().item()  # 计算 MAE\n",
    "            total_mae += mae\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "    avg_mae = total_mae / num_samples\n",
    "    print(f\"Evaluation - MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# 主运行逻辑\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # 定义不同的增强方法\n",
    "    wordnet_transformation = WordSwapWordNet()\n",
    "    #embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "    backtranslate_transformation = BackTranslation(chained_back_translation=2)\n",
    "    #extendword_transformation = WordSwapExtend()\n",
    "    randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "    homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "    randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "    # 组合多个增强方法，并指定执行概率\n",
    "    random_composite_transformation = CompositeAugmenter(\n",
    "        transformations=[\n",
    "            backtranslate_transformation, \n",
    "            homoglyphswap_transformation, \n",
    "            wordnet_transformation, \n",
    "            randomwordsubs_transformation,\n",
    "            randomcharinsert_transformation\n",
    "        ],\n",
    "        probabilities=[1, 0.5, 0.5, 0.1, 0.1]  # 执行概率\n",
    "    )\n",
    "\n",
    "    # 定义约束，避免对停用词进行修改，防止重复修改\n",
    "    constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "    # 语义相似性约束\n",
    "    semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "    constraints.append(semantic_constraint)\n",
    "\n",
    "    # 创建增强器\n",
    "    text_augmenter = Augmenter(\n",
    "        transformation=random_composite_transformation,\n",
    "        constraints=constraints,\n",
    "        pct_words_to_swap=0.1,\n",
    "        transformations_per_example=3  # 生成 3 个不同版本的增强文本\n",
    "    )\n",
    "\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('data/mes_all.csv')\n",
    "    data['text'] = data['title'] + ' ' + data['description']\n",
    "    texts = data['text'].value.to_list()\n",
    "\n",
    "    # 预训练数据集\n",
    "    dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=512, augmenter=text_augmenter)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "    # 预训练 SimSiam\n",
    "    criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    train_simsiam(simsiam, dataloader, criterion, optimizer, device)\n",
    "\n",
    "    # **移除 projector 和 predictor**\n",
    "    simsiam.remove_projection_head()\n",
    "\n",
    "    # 划分数据集\n",
    "    labeled_data = data[data['storypoint'] != -1]\n",
    "    split_idx = int(len(labeled_data) * 0.8)\n",
    "\n",
    "    fine_tune_data = labeled_data.iloc[:split_idx]\n",
    "\n",
    "    fine_tune_texts = fine_tune_data['text'].values.to_list()\n",
    "    fine_tune_labels = fine_tune_data['storypoint'].values.to_list()\n",
    "\n",
    "    # 微调（回归任务）\n",
    "    labeled_dataset = TextDataset(fine_tune_texts, fine_tune_labels, tokenizer=tokenizer, augment=False)\n",
    "    labeled_dataloader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    regression_criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    fine_tune(simsiam, labeled_dataloader, regression_criterion, optimizer, device)\n",
    "\n",
    "    test_data = labeled_data.iloc[split_idx:]\n",
    "    test_texts = test_data['text'].values.to_list()\n",
    "    test_labels = test_data['storypoint'].values.to_list()\n",
    "\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer=tokenizer, augment=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate(simsiam, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# 定义不同的增强方法\n",
    "wordnet_transformation = WordSwapWordNet()\n",
    "#embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "backtranslate_transformation = BackTranslation(chained_back_translation=2)\n",
    "#extendword_transformation = WordSwapExtend()\n",
    "randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "# 组合多个增强方法，并指定执行概率\n",
    "random_composite_transformation = CompositeAugmenter(\n",
    "    transformations=[\n",
    "        backtranslate_transformation, \n",
    "        homoglyphswap_transformation, \n",
    "        wordnet_transformation, \n",
    "        randomwordsubs_transformation,\n",
    "        randomcharinsert_transformation\n",
    "    ],\n",
    "    probabilities=[1, 0.5, 0.5, 0.1, 0.1]  # 执行概率\n",
    ")\n",
    "\n",
    "# 定义约束，避免对停用词进行修改，防止重复修改\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "# 语义相似性约束\n",
    "semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "constraints.append(semantic_constraint)\n",
    "\n",
    "# 创建增强器\n",
    "text_augmenter = Augmenter(\n",
    "    transformation=random_composite_transformation,\n",
    "    constraints=constraints,\n",
    "    pct_words_to_swap=0.1,\n",
    "    transformations_per_example=3  # 生成 3 个不同版本的增强文本\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv('../data/story_point/bamboo.csv')\n",
    "data = data[:10]\n",
    "data['text'] = data['title']\n",
    "texts = data['text'].values.tolist()\n",
    "\n",
    "# 预训练数据集\n",
    "dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=128, augmenter=text_augmenter)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "# 预训练 SimSiam\n",
    "criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "train_simsiam(simsiam, dataloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除 projector 和 predictor\n",
    "simsiam.remove_projection_head()\n",
    "\n",
    "# 划分数据集\n",
    "labeled_data = data[data['storypoint'] != -1]\n",
    "split_idx = int(len(labeled_data) * 0.8)\n",
    "\n",
    "fine_tune_data = labeled_data.iloc[:split_idx]\n",
    "\n",
    "fine_tune_texts = fine_tune_data['text'].values.tolist()\n",
    "fine_tune_labels = fine_tune_data['storypoint'].values.tolist()\n",
    "\n",
    "# 微调（回归任务）\n",
    "labeled_dataset = TextDataset(fine_tune_texts, fine_tune_labels, tokenizer=tokenizer, augment=False)\n",
    "labeled_dataloader = DataLoader(labeled_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "regression_criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "fine_tune(simsiam, labeled_dataloader, regression_criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = labeled_data.iloc[split_idx:]\n",
    "test_texts = test_data['text'].values.tolist()\n",
    "test_labels = test_data['storypoint'].values.tolist()\n",
    "\n",
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer=tokenizer, augment=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 评估模型\n",
    "evaluate(simsiam, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
