{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading text data with optional augmentation.\"\"\"\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_length=128, augment=False):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "        if self.augment:\n",
    "            self.text_views1 = [self.back_translate(text) for text in self.texts]\n",
    "            self.text_views2 = [self.back_translate(text) for text in self.texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            text_view1 = self.text_views1[idx]\n",
    "            text_view2 = self.text_views2[idx]\n",
    "        else:\n",
    "            text_view1 = text\n",
    "\n",
    "        inputs1 = self.tokenizer(\n",
    "            text_view1,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs1 = {key: val.squeeze(0) for key, val in inputs1.items()}\n",
    "\n",
    "        if self.augment:\n",
    "            inputs2 = self.tokenizer(\n",
    "                text_view2,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs2 = {key: val.squeeze(0) for key, val in inputs2.items()}\n",
    "            return inputs1, inputs2\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return inputs1, label\n",
    "        return inputs1\n",
    "\n",
    "    def back_translate(self, text):\n",
    "        \"\"\"Mock back translation for augmentation.\"\"\"\n",
    "        return text[::-1]  # 反转字符串（可替换为真实的回译方法）\n",
    "\n",
    "class SimSiamText(nn.Module):\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=512):\n",
    "        super(SimSiamText, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(768, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim, dim, bias=True),\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(dim, pred_dim, bias=False),\n",
    "            nn.BatchNorm1d(pred_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(pred_dim, dim, bias=True),\n",
    "        )\n",
    "        self.regressor = nn.Linear(768, 1)  # 线性回归头\n",
    "\n",
    "    def forward(self, x1, x2, regression=False):\n",
    "        \"\"\"\n",
    "          定义模型的前向传播过程\n",
    "          x1 和 x2 是同一批图像的两个不同增强视图。\n",
    "          z1 和 z2 是编码器对 x1 和 x2 的编码结果。\n",
    "          p1 和 p2 是预测器对 z1 和 z2 的预测结果。\n",
    "        \"\"\"\n",
    "        if regression:\n",
    "            return self.regressor(self.encoder(x1)[\"pooler_output\"])  # 仅用于回归\n",
    "        z1 = self.projector(self.encoder(x1)[\"pooler_output\"])\n",
    "        z2 = self.projector(self.encoder(x2)[\"pooler_output\"])\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        # z1.detach() 和 z2.detach() 表示在反向传播时不计算 z1 和 z2 的梯度，因为它们只作为目标使用。\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "    \n",
    "    def remove_projection_head(self):\n",
    "        \"\"\"移除 projector 和 predictor 以用于微调\"\"\"\n",
    "        self.projector = None\n",
    "        self.predictor = None\n",
    "\n",
    "\n",
    "def train_simsiam(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"训练 SimSiam 并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(10):  # 预训练 10 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs1 = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            inputs2 = {key: val.to(device) for key, val in batch[1].items()}\n",
    "\n",
    "            p1, p2, z1, z2 = model(inputs1, inputs2)\n",
    "            loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Epoch [{epoch+1}/10], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/10] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def fine_tune(model, dataloader, criterion, optimizer, device, print_freq=10):\n",
    "    \"\"\"微调 SimSiam 进行回归任务，并打印训练进度和损失\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(5):  # 微调 5 轮\n",
    "        epoch_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()  # 取回归输出\n",
    "            loss = criterion(preds, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % print_freq == 0:\n",
    "                print(f\"Fine-tune Epoch [{epoch+1}/5], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Fine-tune Epoch [{epoch+1}/5] Completed | Avg Loss: {avg_loss:.4f} | Time: {elapsed_time:.2f}s\\n\")\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"评估模型在测试集上的 MAE 误差\"\"\"\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch[0].items()}\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            preds = model(inputs, regression=True).squeeze()\n",
    "            mae = torch.abs(preds - labels).sum().item()  # 计算 MAE\n",
    "            total_mae += mae\n",
    "            num_samples += labels.size(0)\n",
    "\n",
    "    avg_mae = total_mae / num_samples\n",
    "    print(f\"Evaluation - MAE: {avg_mae:.4f}\")\n",
    "\n",
    "# 主运行逻辑\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # 示例数据\n",
    "    texts = [\"This is a positive example.\", \"Another negative sample.\"] * 100\n",
    "    labels = torch.tensor([0.9, 0.2] * 100)  # 回归标签\n",
    "\n",
    "    # 预训练数据集\n",
    "    dataset = TextDataset(texts, tokenizer=tokenizer, augment=True, max_length=512)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # 初始化模型\n",
    "    simsiam = SimSiamText(base_encoder=bert_model).to(device)\n",
    "\n",
    "    # 预训练 SimSiam\n",
    "    criterion = nn.CosineSimilarity(dim=1).to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    train_simsiam(simsiam, dataloader, criterion, optimizer, device)\n",
    "\n",
    "    # **移除 projector 和 predictor**\n",
    "    simsiam.remove_projection_head()\n",
    "\n",
    "    # 微调（回归任务）\n",
    "    labeled_dataset = TextDataset(texts, labels, tokenizer=tokenizer, augment=False)\n",
    "    labeled_dataloader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    regression_criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(simsiam.parameters(), lr=3e-4)\n",
    "    fine_tune(simsiam, labeled_dataloader, regression_criterion, optimizer, device)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate(simsiam, labeled_dataloader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
