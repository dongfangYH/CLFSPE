{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textattack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 同义词替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Augmented Sentence: ['The quick brown fox jumps over the lazy pawl.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import WordNetAugmenter\n",
    " \n",
    "augmenter = WordNetAugmenter()\n",
    " \n",
    "# Example usage:\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "augmented_sentence = augmenter.augment(sentence)\n",
    " \n",
    "print(f\"Original Sentence: {sentence}\")\n",
    "print(f\"Augmented Sentence: {augmented_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 回译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: [Audit] Password expiration change is not included in the Audit\n",
      "Augmented texts: ['[Audit] Change in password is not included in the audit']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import BackTranslationAugmenter\n",
    "\n",
    "# 初始化回译增强器\n",
    "augmenter = BackTranslationAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"[Audit] Password expiration change is not included in the Audit\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 随机字符替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['This is a movie!', 'This is a movie great!', 'This is a amp great movie!', 'This is a bang-up movie!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EasyDataAugmenter\n",
    "\n",
    "# 初始化随机字符替换增强器\n",
    "augmenter = EasyDataAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 上下文替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['film is a great movie!']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CLAREAugmenter\n",
    "\n",
    "# 初始化上下文替换增强器\n",
    "augmenter = CLAREAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 随机删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['This is a great!']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import DeletionAugmenter\n",
    "\n",
    "# 初始化随机删除增强器\n",
    "augmenter = DeletionAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 简易数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['This bully is a great movie!', 'is a great movie!', 'This is a enceinte movie!', 'This movie a great is!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EasyDataAugmenter\n",
    "\n",
    "# 初始化随机插入增强器\n",
    "augmenter = EasyDataAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 随机替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['This is a gRreat movie!']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CharSwapAugmenter\n",
    "\n",
    "# 初始化随机交换增强器\n",
    "augmenter = CharSwapAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. checkList数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a great movie!\n",
      "Augmented texts: ['This is a great movie!']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import CheckListAugmenter\n",
    "\n",
    "# 初始化随机交换增强器\n",
    "augmenter = CheckListAugmenter()\n",
    "\n",
    "# 对文本进行增强\n",
    "text = \"This is a great movie!\"\n",
    "augmented_texts = augmenter.augment(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Augmented texts:\", augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Embedding增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: TextAttack is a powerful library for NLP.\n",
      "Augmented Text: ['TextAttack is a powerful bookstores for NLP.']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "\n",
    "# Initialize the EmbeddingAugmenter\n",
    "embed_aug = EmbeddingAugmenter()\n",
    "\n",
    "# Example usage:\n",
    "original_text = \"TextAttack is a powerful library for NLP.\"\n",
    "augmented_text = embed_aug.augment(original_text)\n",
    "\n",
    "print(f\"Original Text: {original_text}\")\n",
    "print(f\"Augmented Text: {augmented_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: Hello, how are you?\n",
      "Back translated texts: Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "from BackTranslator import BackTranslator\n",
    "\n",
    "translator = BackTranslator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: are you ok?\n",
      "Back translated texts: Are you okay?\n"
     ]
    }
   ],
   "source": [
    "# Example text to translate\n",
    "texts = [\"are you ok?\"]\n",
    "\n",
    "# Perform back translation\n",
    "back_translated_texts = translator.back_translate(texts, language_src=\"en\", language_dst=\"es\")\n",
    "print(\"Original texts:\", texts[0])\n",
    "print(\"Back translated texts:\", back_translated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Text 1: The quick brown befuddle jumps over the lazy dog.\n",
      "Augmented Text 2: The quick brownness fox jumps over the lazy dog.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapWordNet, WordSwapEmbedding\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.transformations import CompositeTransformation\n",
    "\n",
    "# 定义数据增强器\n",
    "wordnet_transformation = WordSwapWordNet()\n",
    "embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "\n",
    "# 组合多个替换方法\n",
    "composite_transformation = CompositeTransformation([wordnet_transformation, embedding_transformation])\n",
    "\n",
    "# 定义约束，避免停用词修改，防止重复修改\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "# 添加语义约束，确保替换后的词语与原始词语的相似度\n",
    "semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "constraints.append(semantic_constraint)\n",
    "\n",
    "# 组合增强器\n",
    "combined_augmenter = Augmenter(\n",
    "    transformation=composite_transformation,  # 组合多个转换\n",
    "    constraints=constraints,\n",
    "    pct_words_to_swap=0.1,\n",
    "    transformations_per_example=2\n",
    ")\n",
    "\n",
    "# 示例文本\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# 生成增强文本\n",
    "augmented_texts = combined_augmenter.augment(text)\n",
    "for i, augmented_text in enumerate(augmented_texts):\n",
    "    print(f\"Augmented Text {i+1}: {augmented_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\henry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "c:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapWordNet, WordSwapEmbedding, BackTranslation, WordSwapExtend, WordSwapRandomCharacterSubstitution, WordSwapHomoglyphSwap, WordSwapRandomCharacterInsertion\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.transformations.composite_transformation import CompositeTransformation\n",
    "\n",
    "class RandomCompositeTransformation(CompositeTransformation):\n",
    "    \"\"\"\n",
    "    允许以一定概率执行多个数据增强方法的组合转换。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transformations, probabilities):\n",
    "        \"\"\"\n",
    "        :param transformations: 一个列表，包含多个 Transformation 对象（如 WordNet 替换、Embedding 替换）。\n",
    "        :param probabilities: 一个列表，包含每个转换被选择的概率（总和不需要为 1）。\n",
    "        \"\"\"\n",
    "        assert len(transformations) == len(probabilities), \"transformations 和 probabilities 长度必须相同\"\n",
    "        super().__init__(transformations)\n",
    "        self.probabilities = probabilities\n",
    "\n",
    "    def _get_transformations(self, current_text, indices_to_modify=None):\n",
    "        \"\"\"按照概率随机选择要执行的转换\"\"\"\n",
    "        selected_transformations = [\n",
    "            t for t, p in zip(self.transformations, self.probabilities) if random.random() < p\n",
    "        ]\n",
    "        if not selected_transformations:\n",
    "            selected_transformations = [random.choice(self.transformations)]  # 确保至少执行一个\n",
    "        \n",
    "        # 应用选中的转换\n",
    "        transformed_texts = []\n",
    "        for transformation in selected_transformations:\n",
    "            transformed_texts.extend(transformation(current_text, indices_to_modify))\n",
    "        return transformed_texts\n",
    "\n",
    "# 定义不同的增强方法\n",
    "wordnet_transformation = WordSwapWordNet()\n",
    "embedding_transformation = WordSwapEmbedding(max_candidates=5)\n",
    "backtranslate_transformation = BackTranslation(chained_back_translation=2)\n",
    "extendword_transformation = WordSwapExtend()\n",
    "randomwordsubs_transformation = WordSwapRandomCharacterSubstitution()\n",
    "homoglyphswap_transformation = WordSwapHomoglyphSwap()\n",
    "randomcharinsert_transformation = WordSwapRandomCharacterInsertion()\n",
    "\n",
    "# 组合多个增强方法，并指定执行概率\n",
    "random_composite_transformation = RandomCompositeTransformation(\n",
    "    transformations=[\n",
    "        backtranslate_transformation, \n",
    "        homoglyphswap_transformation, \n",
    "        wordnet_transformation, \n",
    "        randomwordsubs_transformation,\n",
    "        randomcharinsert_transformation\n",
    "    ],\n",
    "    probabilities=[1, 0.5, 0.5, 0.5, 0.1]  # 执行概率\n",
    ")\n",
    "\n",
    "# 定义约束，避免对停用词进行修改，防止重复修改\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "# 语义相似性约束\n",
    "semantic_constraint = WordEmbeddingDistance(min_cos_sim=0.8)\n",
    "constraints.append(semantic_constraint)\n",
    "\n",
    "# 创建增强器\n",
    "probabilistic_augmenter = Augmenter(\n",
    "    transformation=random_composite_transformation,\n",
    "    constraints=constraints,\n",
    "    pct_words_to_swap=0.1,\n",
    "    transformations_per_example=3  # 生成 3 个不同版本的增强文本\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\henry\\Workspace\\deepcluster\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4087: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Text 1: [FE][Page Config] Creating a single-state-manager page or copying a PWB page breaas the Select Station widget\n",
      "Augmented Text 2: [FE][Page Config] Creating a single-state-manager page or copying a PWB page intermit the Select Station widget\n",
      "Augmented Text 3: [FE][Page Config] Creating a single-state-manager page or copying a PWB pagе breaks the Select Station widget\n"
     ]
    }
   ],
   "source": [
    "# 示例文本\n",
    "text = \"[FE][Page Config] Creating a single-state-manager page or copying a PWB page breaks the Select Station widget\"\n",
    "\n",
    "# 生成增强文本\n",
    "augmented_texts = probabilistic_augmenter.augment(text)\n",
    "for i, augmented_text in enumerate(augmented_texts):\n",
    "    print(f\"Augmented Text {i+1}: {augmented_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[FE][Page Config] Creating a single-state-manager page or copying a PWB page breaas the Select Station widget'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(augmented_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
